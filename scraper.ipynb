{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a486c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# /home/ubuntu/LiveFetch/scraper.py\n",
    "DEPLOYED = False  # Set to True when deploying to production\n",
    "\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import tempfile\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options as chromeOptions\n",
    "from selenium.webdriver.edge.options import Options as EdgeOptions\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException, StaleElementReferenceException\n",
    "\n",
    "# --- Configuration ---\n",
    "TARGET_URL = \"https://www.radheexch.xyz/game/4\"\n",
    "JSON_OUTPUT_FILE = \"/app/data/live_data.json\" if DEPLOYED else \"./data/live_data.json\" # Must match JSON_OUTPUT_FILE in api_server.py\n",
    "SCRAPE_INTERVAL_SECONDS = 2\n",
    "WEB_DRIVER_TIMEOUT = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cca6bf01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Selenium Driver Setup ---\n",
    "\n",
    "def setup_driver():\n",
    "    \"\"\"Initializes and returns a headless Selenium WebDriver.\"\"\"\n",
    "    try:\n",
    "        options = chromeOptions() if DEPLOYED else EdgeOptions()\n",
    "        # options.add_argument(\"--headless\")\n",
    "        options.add_argument(\"--no-sandbox\")\n",
    "        options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.102 Safari/537.36\")\n",
    "        \n",
    "        # --- DNS Configuration ---\n",
    "        local_state = {\n",
    "            \"dns_over_https.mode\": \"secure\",\n",
    "            \"dns_over_https.templates\": \"https://chrome.cloudflare-dns.com/dns-query\",\n",
    "        }\n",
    "        options.add_experimental_option('localState', local_state)\n",
    "\n",
    "        driver = webdriver.Chrome(options=options) if DEPLOYED else webdriver.Edge(options=options) # type: ignore\n",
    "            \n",
    "        driver.set_page_load_timeout(WEB_DRIVER_TIMEOUT)\n",
    "        return driver\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing WebDriver: {e}\")\n",
    "        return None\n",
    "    \n",
    "# --- JSON File Handling ---\n",
    "\n",
    "def write_to_json(data, filename):\n",
    "    \"\"\"Atomically writes data to a JSON file.\"\"\"\n",
    "    temp_path = None\n",
    "    try:\n",
    "        os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "        temp_dir = os.path.dirname(filename)\n",
    "        with tempfile.NamedTemporaryFile('w', delete=False, dir=temp_dir, encoding='utf-8') as temp_f:\n",
    "            json.dump(data, temp_f, indent=4)\n",
    "            temp_path = temp_f.name\n",
    "        \n",
    "        os.replace(temp_path, filename)\n",
    "        print(f\"Successfully updated {filename}\")\n",
    "    except (IOError, os.error, json.JSONDecodeError) as e:\n",
    "        print(f\"Error writing to JSON file {filename}: {e}\")\n",
    "        if temp_path and os.path.exists(temp_path):\n",
    "            os.remove(temp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72fbc2c",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# --- Scraping Functions ---\n",
    "\n",
    "def get_live_match_count(driver):\n",
    "    \"\"\"Finds all live matches and returns their count.\"\"\"\n",
    "    try:\n",
    "        match_table_xpath = \"//*[@id='root']/body/div[7]/div[2]/div[2]/div[2]/table/tbody\"\n",
    "        wait = WebDriverWait(driver, WEB_DRIVER_TIMEOUT)\n",
    "        tbody = wait.until(EC.presence_of_element_located((By.XPATH, match_table_xpath)))\n",
    "        \n",
    "        # Find rows that are live\n",
    "        live_rows = tbody.find_elements(By.CSS_SELECTOR, \"tr .livenownew\")\n",
    "        return len(live_rows)\n",
    "        \n",
    "    except (TimeoutException, NoSuchElementException):\n",
    "        print(\"Could not find match table to count live matches.\")\n",
    "        return 0\n",
    "    except Exception as e:\n",
    "        print(f\"Error in get_live_match_count: {e}\")\n",
    "        return 0\n",
    "\n",
    "def get_live_match_data(driver):\n",
    "    \"\"\"\n",
    "    Scrapes Bookmaker, Fancy, and Session odds from the *current* page.\n",
    "    This function assumes the driver is already on the match detail page.\n",
    "    \"\"\"\n",
    "    wait = WebDriverWait(driver, WEB_DRIVER_TIMEOUT)\n",
    "    \n",
    "    # --- Scrape Bookmaker ---\n",
    "    bookmaker_data = []\n",
    "    try:\n",
    "        bookmaker_xpath = \"//*[@id='root']/body/div[7]/div[2]/div/div[4]/div[2]\"\n",
    "        bookmaker_container = wait.until(EC.presence_of_element_located((By.XPATH, bookmaker_xpath)))\n",
    "        team_rows = bookmaker_container.find_elements(By.XPATH, \".//table/tbody/tr[position() > 1 and count(td) > 1]\")\n",
    "        \n",
    "        for row in team_rows:\n",
    "            team_name = row.find_element(By.CSS_SELECTOR, \"span.in-play-title\").text\n",
    "            \n",
    "            back_prices = []\n",
    "            back_elements = row.find_elements(By.CSS_SELECTOR, \"a.btn-back\")\n",
    "            for el in back_elements:\n",
    "                price = el.find_element(By.CSS_SELECTOR, \"div\").text.strip()\n",
    "                size = el.find_element(By.CSS_SELECTOR, \"span\").text.strip()\n",
    "                if price:\n",
    "                    back_prices.append({\"price\": price, \"size\": size})\n",
    "\n",
    "            lay_prices = []\n",
    "            lay_elements = row.find_elements(By.CSS_SELECTOR, \"a.btn-lay\")\n",
    "            for el in lay_elements:\n",
    "                price = el.find_element(By.CSS_SELECTOR, \"div\").text.strip()\n",
    "                size = el.find_element(By.CSS_SELECTOR, \"span\").text.strip()\n",
    "                if price:\n",
    "                    lay_prices.append({\"price\": price, \"size\": size})\n",
    "            \n",
    "            bookmaker_data.append({\n",
    "                \"team_name\": team_name,\n",
    "                \"back\": back_prices,\n",
    "                \"lay\": lay_prices\n",
    "            })\n",
    "\n",
    "    except (TimeoutException, NoSuchElementException, Exception) as e:\n",
    "        print(f\"Could not scrape Bookmaker data: {e}\")\n",
    "\n",
    "    # --- Scrape Fancy & Sessions ---\n",
    "    fancy_data = []\n",
    "    session_data = []\n",
    "    try:\n",
    "        fancy_xpath = \"//*[@id='root']/body/div[7]/div[2]/div/div[5]/div/div[4]/table/tbody\"\n",
    "        fancy_container = wait.until(EC.presence_of_element_located((By.XPATH, fancy_xpath)))\n",
    "        \n",
    "        # Select only data rows, skip headers/mobile separators\n",
    "        market_rows = fancy_container.find_elements(By.XPATH, \".//tr[not(contains(@class, 'bet-all-new')) and not(contains(@class, 'brblumobile'))]\")\n",
    "        \n",
    "        for row in market_rows:\n",
    "            market_name = row.find_element(By.CSS_SELECTOR, \"span.marketnamemobile\").text.strip()\n",
    "            \n",
    "            # Find No/Lay values\n",
    "            lay_btn = row.find_element(By.CSS_SELECTOR, \"a.btn-lay\")\n",
    "            no_val = lay_btn.find_element(By.CSS_SELECTOR, \"div\").text.strip()\n",
    "            no_size = lay_btn.find_element(By.CSS_SELECTOR, \"span\").text.strip()\n",
    "\n",
    "            # Find Yes/Back values\n",
    "            back_btn = row.find_element(By.CSS_SELECTOR, \"a.btn-back\")\n",
    "            yes_val = back_btn.find_element(By.CSS_SELECTOR, \"div\").text.strip()\n",
    "            yes_size = back_btn.find_element(By.CSS_SELECTOR, \"span\").text.strip()\n",
    "            \n",
    "            market_item = {\n",
    "                \"name\": market_name, \n",
    "                \"no_val\": no_val, \n",
    "                \"no_size\": no_size, \n",
    "                \"yes_val\": yes_val, \n",
    "                \"yes_size\": yes_size\n",
    "            }\n",
    "\n",
    "            if \"over\" in market_name.lower() or \"run\" in market_name.lower():\n",
    "                session_data.append(market_item)\n",
    "            else:\n",
    "                fancy_data.append(market_item)\n",
    "                \n",
    "    except (TimeoutException, NoSuchElementException, Exception) as e:\n",
    "        print(f\"Could not scrape Fancy/Session data: {e}\")\n",
    "\n",
    "    # --- Scrape Result ---\n",
    "    result = \"In Progress\" # Default\n",
    "    try:\n",
    "        # --- PLACEHOLDER SELECTOR ---\n",
    "        result_element = driver.find_element(By.CSS_SELECTOR, \"div.match-result-text\")\n",
    "        result = result_element.text\n",
    "    except NoSuchElementException:\n",
    "        pass # No result yet, keep default\n",
    "    \n",
    "    return {\n",
    "        \"bookmaker\": bookmaker_data,\n",
    "        \"fancy\": fancy_data,\n",
    "        \"sessions\": session_data,\n",
    "        \"result\": result,\n",
    "        \"last_updated\": time.time()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f3dcde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Main Application Loop ---\n",
    "\n",
    "def main_loop():\n",
    "    \"\"\"Main scraping loop.\"\"\"\n",
    "    driver = setup_driver()\n",
    "    if not driver:\n",
    "        print(\"Failed to start driver. Exiting.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            print(\"=\"*30)\n",
    "            print(f\"Starting new scrape cycle at {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "            \n",
    "            all_live_match_data = []\n",
    "            \n",
    "            try:\n",
    "                driver.get(TARGET_URL)\n",
    "                live_match_count = get_live_match_count(driver)\n",
    "                \n",
    "                if live_match_count == 0:\n",
    "                    print(\"No live matches found. Retrying...\")\n",
    "                    write_to_json([], JSON_OUTPUT_FILE) # Write empty list\n",
    "                    time.sleep(SCRAPE_INTERVAL_SECONDS)\n",
    "                    continue\n",
    "\n",
    "                print(f\"Found {live_match_count} live matches. Processing...\")\n",
    "\n",
    "                # Loop from 0 to count-1\n",
    "                for i in range(live_match_count):\n",
    "                    print(f\"Processing match {i + 1} of {live_match_count}...\")\n",
    "                    \n",
    "                    # Re-find the match table to avoid StaleElementReferenceException\n",
    "                    match_table_xpath = \"//*[@id='root']/body/div[7]/div[2]/div[2]/div[2]/table/tbody\"\n",
    "                    wait = WebDriverWait(driver, WEB_DRIVER_TIMEOUT)\n",
    "                    \n",
    "                    try:\n",
    "                        tbody = wait.until(EC.presence_of_element_located((By.XPATH, match_table_xpath)))\n",
    "                        # Find all 'tr' elements that contain a 'livenownew' class\n",
    "                        live_rows = tbody.find_elements(By.XPATH, \".//tr[.//div[contains(@class, 'livenownew')]]\")\n",
    "                        \n",
    "                        if i >= len(live_rows):\n",
    "                            print(\"Match index out of bounds, list may have changed. Restarting cycle.\")\n",
    "                            break\n",
    "                            \n",
    "                        row_to_click = live_rows[i]\n",
    "                        \n",
    "                        # Get teams text *before* clicking\n",
    "                        teams_text = row_to_click.find_element(By.CSS_SELECTOR, \".event-title\").text\n",
    "                        # Clean up teams text: \"28 Oct 08:00 | Western Australia v South Australia\" -> \"Western Australia v South Australia\"\n",
    "                        if \"|\" in teams_text:\n",
    "                            teams = teams_text.split(\"|\", 1)[-1].strip()\n",
    "                        else:\n",
    "                            teams = teams_text.strip()\n",
    "                        \n",
    "                        # Click the event info cell to navigate\n",
    "                        clickable_cell = row_to_click.find_element(By.CSS_SELECTOR, \"td.eventInfo\")\n",
    "                        clickable_cell.click()\n",
    "                        \n",
    "                        # Wait for URL to change to the event page\n",
    "                        wait.until(EC.url_contains(\"/event/\"))\n",
    "                        \n",
    "                        # Get Match ID from URL\n",
    "                        current_url = driver.current_url\n",
    "                        match_id = current_url.split('/')[-1].split('?')[0] # Get last part of URL, remove queries\n",
    "                        \n",
    "                        print(f\"Scraping data for: {teams} (ID: {match_id})\")\n",
    "\n",
    "                        # Scrape the detailed data\n",
    "                        live_data = get_live_match_data(driver)\n",
    "                        live_data[\"match_id\"] = match_id\n",
    "                        live_data[\"teams\"] = teams\n",
    "                        all_live_match_data.append(live_data)\n",
    "\n",
    "                    except (StaleElementReferenceException, TimeoutException, NoSuchElementException) as e:\n",
    "                        print(f\"Error processing match {i + 1}: {e}. Skipping.\")\n",
    "                    \n",
    "                    # Go back to the main URL to process the next match\n",
    "                    print(\"Navigating back to match list...\")\n",
    "                    driver.get(TARGET_URL)\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error in main scraping cycle: {e}\")\n",
    "\n",
    "            # Write all collected data to the JSON file\n",
    "            write_to_json(all_live_match_data, JSON_OUTPUT_FILE)\n",
    "\n",
    "            # Step 4: Repeat after interval\n",
    "            print(f\"Cycle complete. Waiting {SCRAPE_INTERVAL_SECONDS} seconds...\")\n",
    "            print(\"=\"*30)\n",
    "            time.sleep(SCRAPE_INTERVAL_SECONDS)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nScraping stopped by user.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An uncaught error occurred in main loop: {e}\")\n",
    "    finally:\n",
    "        if driver:\n",
    "            print(\"Shutting down WebDriver.\")\n",
    "            driver.quit()\n",
    "\n",
    "\n",
    "main_loop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
