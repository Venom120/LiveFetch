# LiveFetch

LiveFetch is a Python-based application designed to scrape live sports data from a target website and an external API, process this data, and then serve it via a RESTful API. It leverages Selenium for dynamic web content scraping and `requests` for efficient API interactions, all managed within a multi-threaded architecture.

## Features

- **Live Data Scraping**: Continuously scrapes real-time match odds, bookmaker information, fancy bets, and session data.
- **Dual Data Sources**: Combines data from a target website (using Selenium) and a third-party API (using `requests`).
- **Multi-threaded Architecture**: Efficiently manages scraping of multiple live matches concurrently using a main manager thread and dedicated worker threads for each match.
- **RESTful API**: Provides a `/api/livedata` endpoint to serve the aggregated live data in JSON format.
- **Configurable**: Easy-to-use `settings.json` for customizing scraping intervals, target URLs, API endpoints, and deployment settings.
- **Docker Support**: Includes a `DOCKERFILE` for containerized deployment, ensuring a consistent and isolated environment.

## Components

- [`scraper.py`](scraper.py): The core scraping logic. It fetches market lists from an API, identifies live matches, and spawns worker threads to scrape detailed fancy and session data using Selenium. It then aggregates this data and writes it to a JSON file.
- [`api_server.py`](api_server.py): A Flask application that exposes the scraped data. It reads the `live_data.json` file (generated by `scraper.py`) and serves its content through the `/api/livedata` endpoint.
- [`settings.json`](settings.json): Configuration file for the application, managing various parameters for both the scraper and the API server.
- [`requirements.txt`](requirements.txt): Lists all Python dependencies required for the project.
- [`DOCKERFILE`](DOCKERFILE): Defines the Docker image for the application, including the installation of Python, Chromium, and project dependencies.

## Getting Started

### Prerequisites

- Python 3.8+
- Docker (for containerized deployment)
- `settings.json` configured with appropriate URLs and keys.

### Local Setup

1.  **Clone the repository:**
    ```bash
    git clone https://github.com/Venom120/LiveFetch.git
    cd LiveFetch
    ```

2.  **Install dependencies:**
    ```bash
    pip install -r requirements.txt
    ```

3.  **Configure `settings.json`:**
    Create or update the `settings.json` file in the root directory. An example structure is provided below:
    ```json
    {
      "DEFAULT": {
        "DEPLOYED": false,
        "LEVEL": "INFO"
      },
      "Scraper": {
        "HEADLESS": true,
        "TARGET_URL": "https://www.radheexch.xyz",
        "SCRAPE_INTERVAL_SECONDS": 2,
        "LIST_REFRESH_INTERVAL_SECONDS": 10,
        "WEB_DRIVER_TIMEOUT": 10
      },
      "Api": {
        "AUTH_URL": "https://api.radheexch.xyz/sso/auth/demo",
        "MARKET_URL": "https://api.radheexch.xyz/marketprovider/markets/eventtype/4",
        "OP_KEY": "RDE"
      },
      "Paths": {
        "DATA_DIR": "./data",
        "DEPLOYED_DATA_DIR": "/app/data",
        "DATA_FILE": "live_data.json"
      }
    }
    ```
    Set `"DEPLOYED": false` for local development.

4.  **Create data directory:**
    ```bash
    mkdir -p data
    ```

5.  **Run the scraper:**
    ```bash
    python scraper.py
    ```
    This will start scraping data and writing it to `./data/live_data.json`.

6.  **Run the API server:**
    In a separate terminal:
    ```bash
    python api_server.py
    ```
    The API will be available at `http://0.0.0.0:5100/api/livedata`.

### Docker Deployment

1.  **Build the Docker image:**
    ```bash
    docker build -t livefetch .
    ```

2.  **Run the scraper container:**
    ```bash
    docker run -d --name livefetch-scraper -v $(pwd)/data:/app/data livefetch python scraper.py
    ```

3.  **Run the API server container:**
    ```bash
    docker run -d --name livefetch-api -p 5100:5100 -v $(pwd)/data:/app/data livefetch python api_server.py
    ```

4.  **Access the API:**
    The API will be available at `http://localhost:5100/api/livedata`.

## API Endpoint

-   **GET `/api/livedata`**: Returns the currently scraped live data in JSON format.
    -   **Response**: A JSON array of match objects, each containing odds, bookmaker, fancy, and session data.
    -   **Example Success Response**:
        ```json
        [
          {
            "match_id": "12345",
            "last_updated": 1678886400,
            "teams": "Team A vs Team B",
            "odds": [
              {
                "team_name": "Team A",
                "back": [{"price": "2.5", "size": "100"}],
                "lay": [{"price": "2.6", "size": "90"}]
              }
            ],
            "bookmarker": [],
            "fancy": [],
            "sessions": [],
            "in_play": true,
            "result": "In Progress"
          }
        ]
        ```
    -   **Error Responses**:
        -   `404 Not Found`: If `live_data.json` is not found (scraper may not have run).
        -   `500 Internal Server Error`: If there's an error decoding JSON or an unexpected server error.

## Project Structure

```
.
├── api_server.py           # Flask API to serve live data
├── scraper.py              # Main scraping logic
├── settings.json           # Configuration file
├── requirements.txt        # Python dependencies
├── DOCKERFILE              # Docker build instructions
├── README.md               # Project README
├── LICENSE                 # MIT License
├── data/                   # Directory for scraped data
│   └── live_data.json      # Output file for live data
└── tests/                  # Unit and integration tests
    ├── __init__.py
    ├── conftest.py
    ├── test_api_server.py
    └── test_scraper.py
```

## License

This project is licensed under the MIT License - see the [`LICENSE`](LICENSE) file for details.

## Contributing

Contributions are welcome! Please feel free to submit pull requests or open issues for bugs and feature requests.